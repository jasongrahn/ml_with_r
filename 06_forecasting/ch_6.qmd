---
title: "chapter 6, Forecasting numeric data with regression"
author: "jason grahn"
format: gfm
#output: "asis"
---

## Chapter 6 - forecasting

# Regression

-   **dependent variable**: value to predict

-   **independent variable**: predictors

-   **slope-intercept form**: the old $y = m(x) + B$ or in generic form: $y = a + b(x)$

    -   Slope is the `m` (or `b`) in those formulas.

    -   `B` or `a` is the y-intercept. If no value is supplied, it's `0`.

Different types of regression:

-   linear: straight lines! --- assuming the dependent variable is measured on a continuous scale

    -   simple linear: using a single independent variable

    -   multiple linear (aka "multiple): using 2+ independent variables (instantly more complex)

-   logistic: modeling binary outcomes (Yes / No, True / False, etc)

-   poisson: predicts a count events that occur

-   gamma: used for right-skewed data (like poisson) - models the *time* to an event or *cost* of an event (insurance claim costs for an accident, for example)

All of these are GLMs (generalized linear models)

## Ordinary Least Squares estimation

-   Ordinary Least Squares: method to determine the optimal estimates of `a` and `b` in the generic linear regression formulas.

-   Sum of Squared Errors (SSE) (aka: residuals): vertical distance between predicted `y` values and their actual `y` values. These can be above or below the linear plotting. Squaring this distance makes the values positive.

    -   The total sum of $(y_i - \hat{y}_i)^2$ = the sum of errors squared or $e^2_i$

    -   the "hat" over the Y generally means "an estimate of this value"

Lets go get some data.

```{r}
launch <- 
    read.csv("https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-with-R-Fourth-Edition/main/Chapter%2006/challenger.csv")

launch |> str()
```

```{r}
b <- 
    cov(launch$temperature, 
        launch$distress_ct) / 
    var(launch$temperature)
b
```

```{r}
a <- mean(launch$distress_ct) - b * mean(launch$temperature)

a
```

## Correlations

-   correlation: how closely a relationship follows a straight line.

    -   **pearson's correlation coefficient (R):** a numeric value of correlation, in a range of -1 to +1 where the min and max values reflect "perfect" relationships. A value of 0 is "no relationship"

    -   For data data around humans. +/-0.5 *can* be considered strong.

    -   for mechanical / machine processes, +/-0.5 *can* be considered pretty weak.

```{r}
cor(launch$temperature, launch$distress_ct)
```

## Multiple linear regression

writing a basic regression function...

```{r}
reg <- 
    function(y , x) {
        x <- as.matrix(x) # take a set of values and make a matrix
        x <- cbind(Intercept = 1, x) # create a new column and set values to `1`
        b <- solve(t(x) %*% x) %*% t(x) %*% y 
        # %*% = multiples two matricies
        # t transposes a matrix
        # solve takes the inverse
        colnames(b) <- "estimate"
        print(b)
    }
```

```{r}
str(launch)
```

```{r}
reg(y = launch$distress_ct,
    x = launch[2])
```

interesting - if we specifically call the column name the name doesn't reflect in the estimate table. Only by using the `[]` notation does it carry over. hm.

```{r}
reg(y = launch$distress_ct,
    x = launch[2:4])
```

R has built in linear modeling and I dont know why this author *chose* to write his own function instead of teaching that...

```{r}
# model1 <- 
#     lm(distress_ct ~ temperature + field_check_pressure + flight_num, 
#        data = launch)

model2 <- 
    lm(distress_ct ~ ., 
       data = launch)

# model1
model2
```

## GLM and logistic regression

GLM = Generalized linear models

"For modeling counting values, for categorical or binary outcomes, as well as other cases where the target is not a normally distributed continuous variable, standard linear regression is not the best tool for the job."

GLM "loosens" assumptions of linear modeling:

-   Allows the target (dependent variable) to be non-normally distributed, non-continuous.

-   Allows the the variance of the target to be related to it's mean.

**Logistic regression:** GLM variant that uses binomial distribution with a log link function. The most important form allows regression to be used on binary classifiers.

-   **logit link:** function in the form of $log(p / (1-p))$ where `p` is a probability.

    -   $(p / (1-p)$ expresses the probability as odds. "The probability it happens divided by the probability it doesn't.

-   **odds ratios:** logistic regression coefficients indicate the difference in the odds of `y` due to a one-unit increase in `x`. When odds are exponentiated, odds ratios express the relative increase or decrease in the chances that `y` happens.

The relationship between odds and probability isn't linear. Impact of a change on a failure probability depends on the context in which the change is happening.

Fitted regression models create an s-like curve. Represents a probability estimate on a continuous scale between 0 (no chance) and 1 (gunna happen!)

To get a binary prediction (TRUE/FALSE), define the probability threshold. ex: if probability to renew is greater than .7, then "TRUE" else "FALSE". The higher the threshold, the harder to achieve (obvi).

-   **maximum likelihood estimation (MLE):** GLM technique which finds the parameter values for the distribution that are most likely to have generated the observed data.

# Ex: Predicting claims cost using LR.

## Getting data

let's go get data:

```{r}
insurance <- 
    read.csv("https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-with-R-Fourth-Edition/main/Chapter%2006/autoinsurance.csv",
             stringsAsFactors = TRUE)

insurance |> str()
```

of note are the binary fields `hard_braking_ind` and `late_driving_ind` that reflect bad driving behaviours.

## exploring

The dependent variable - what we're predicting - is "expenses".

Checking for nomality..

```{r}
summary(insurance$expenses)
```

definitely not normal. string right skew.

```{r}
hist(insurance$expenses,
     breaks = 30)
```

```{r}
table(insurance$geo_area)
table(insurance$vehicle_type)
```

## data relationships - the correlation matrix

We use these to see if anything has a high relationship with the dependent variable *or* any other variable

```{r}
cor(insurance[c("age", "est_value", "miles_driven", "expenses")]) |> 
    round(4)
```

## visualizing relationships with scatterplots

-   **SPLOM:** scatterplot matrix. a collection of scatterplots in a grid.

```{r}
pairs(insurance[c("age", "est_value", "miles_driven",
                  "expenses")], 
      pch = ".")
```

Better plotting of the same through `psych`.

```{r}
psych::pairs.panels(insurance[c("age", "est_value", "miles_driven",
                           "expenses")], 
                     pch = ".")
```

## training the model

```{r}
ins_model <- 
    lm(expenses ~ ., 
       data = insurance)

options(scipen = 999)
ins_model

```

Note the factor variables are blown up into individual categories for each level of the factor *minus one.* The missing factor is used as reference point to which the others are compared.

## evaluating model performance

```{r}
summary(ins_model)
```

Not good when it comes to statistical significance for the variables, but yes for the overall model. Those R-squared values are terrible and this model is punished from using too many variables.

Stepwise regression would find the best use here.. http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/154-stepwise-regression-essentials-in-r/#computing-stepwise-regression

```{r}
library(MASS)
# Fit the full model 
full.model <- lm(expenses ~., data = insurance)
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "both", 
                      trace = FALSE)
summary(step.model)
```

But even then, that's a pretty shitty model!

I wonder what the book is going to do next.

### Adding nonlinear relationships!

Things like squaring and square roots are non-linear relationships. Book wants age squared.

```{r}
insurance$age2 <- insurance$age^2

```

### and interaction effects...

interactions are when two variables interact with each other.

```{r}
ins_model2 <- lm(expenses ~ . + hard_braking_ind:late_driving_ind,
                 data = insurance)

summary(ins_model2)

step.model <- stepAIC(ins_model2, direction = "both", 
                      trace = FALSE)
summary(step.model)
```

## making predictions with regression

```{r}
insurance$pred <- predict(ins_model2, insurance)
insurance$step <- predict(step.model, insurance)
```

```{r}
cor(insurance$pred, insurance$expenses)
cor(insurance$step, insurance$expenses)
```

```{r}
plot(insurance$pred, insurance$expenses)
abline(a = 0, b = 1, col = "red", lwd = 3, lty = 2)

```

predicting one person's potential expenses using the book model...

```{r}
predict(ins_model2,
        data.frame(age = 30, age2 = 30^2, geo_area = "rural", 
                   vehicle_type = "truck", est_value = 25000,
                   miles_driven = 14000, college_grad_ind = 0,
                   speeding_ticket_ind = 0, hard_braking_ind = 0,
                   late_driving_ind = 0, clean_driving_ind = 1))
```

# Ex: Predicting churn with logistic regression

yeah.

```{r}
churn_data <- read.csv("https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-with-R-Fourth-Edition/main/Chapter%2006/insurance_churn.csv",
                  stringsAsFactors = TRUE)

churn_data |> str()
```

```{r}
churn_model <- glm(churn ~ . -member_id, 
                   data = churn_data,
                   family = binomial(link = "logit"))
```

These make sense.. Years with, vehicles, having a premium plan, usage of a mobile app, and bundling would all contribute to *staying;* while a recent rate increase is a huge factor for leaving. The **log odds** of a recent rate increase will increase churn by 0.6481 when the rate increase indicator is `TRUE` or `1`.

```{r}
exp(0.648100)
```

The odds ratio says that churn is nearly 2x as likely after a rate increase. Hear that {former company}?

... we're manually calculating this stuff.. It'd be nice if there was a table version of this output so I could log-odds all the estimates ...

The model indicates that autopay may not be a huge contributor.

```{r}
step.model <- stepAIC(churn_model, direction = "both", 
                      trace = FALSE)

summary(step.model)
```

using a stepwise regression removes the autopay feature from the model and slightly decreases the log-odds contribution of the rate increases. Interesting.

```{r}
exp(0.647612)
```

They've supplied a test dataset.

```{r}
churn_test <- read.csv("https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-with-R-Fourth-Edition/main/Chapter%2006/insurance_churn_test.csv")
```

```{r}
churn_test$churn_prob <- predict(churn_model, 
                                 churn_test,
                                 # using type response to get probabilities 
                                 # rather than links
                                 type = "response")
                                 

churn_test$churn_prob2 <- predict(step.model,
                                   churn_test,
                                   # using type response to get probabilities 
                                   # rather than links
                                   type = "response")

summary(churn_test$churn_prob)
summary(churn_test$churn_prob2)
```

Comparing the model outputs against the test data, the stepwise removal of autopay lowered churn rates across the board except for the 1st quartile.

```{r}
churn_order <- order(churn_test$churn_prob, 
                     decreasing = TRUE)

head(churn_test[churn_order, c("member_id", "churn_prob")], 
     n = 5)
```

# regression trees and model trees

trees used for numeric prediction have 2 types:

-   **regression trees:** make predictions based on the average value of examples that reach a leaf.

-   **model trees**: at each leaf, a multiple linear regression model is built from the examples reaching that node. Depending on the number of leaf nodes, a model tree may build tens or even hundreds of such models. more difficult to understand, but tends to have a more accurate model.

This is what I wanted to eventually do with {former company} churn modeling.

I chuckled at this statement from the book in the list of "pros" for these types of models..

[***Does not require knowledge of statistics to interpret the model.***]{.underline}

Like, maybe that's part of the problem with some of the ML folks out there. "I can build a tree!" "Yeah, well, what goes into building it?"

From the book: "...for numeric decision trees, homogeneity is measured by statistics such as variance, standard deviation, or absolute deviation from the mean."

So understanding statistics is *quite* important for using regression & model trees.

-   **standard deviation reduction:** a common splitting criterion used for numeric trees. measures the reduction in standard deviation by comparing the pre-split to the weighted post-split standard deviation.

```{r}
## Example: Calculating SDR ----
# set up the data
tee <- c(1, 1, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7)
at1 <- c(1, 1, 1, 2, 2, 3, 4, 5, 5)
at2 <- c(6, 6, 7, 7, 7, 7)
bt1 <- c(1, 1, 1, 2, 2, 3, 4)
bt2 <- c(5, 5, 6, 6, 7, 7, 7, 7)

# compute the SD Reduction

sdr_a <- sd(tee) - (length(at1) / length(tee) * sd(at1) + length(at2) / length(tee) * sd(at2)) # 2.4 - 1.2
sdr_b <- sd(tee) - (length(bt1) / length(tee) * sd(bt1) + length(bt2) / length(tee) * sd(bt2)) # 2.4 - 1.011

# compare the SDR for each split
sdr_a
sdr_b

```

## ex: estimating wine quality

### getting data..

```{r}
wwine <- read.csv("https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-with-R-Fourth-Edition/main/Chapter%2006/whitewines.csv")

rwines <- read.csv("https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-with-R-Fourth-Edition/main/Chapter%2006/redwines.csv")
# I really should figure out a paramaterized version of this to build a function.
# hm, that _then_ stores the information in SQLlite? some sorta sql? that'd be fun. 
```

```{r}
str(wwine)
```

### prep and simple exploration

benefit of decision trees is that we dont need to worry about normalizing or standardizing features. But we have to evaluate the result data to make sure it fits at least some degree of normality; otherwise we end up with just "Good" or "bad" and end up with a classifier model instead.

```{r}
hist(wwine$quality)
#summary(wwine)
```

divide into training/test data

```{r}
wine_train <- wwine[1:3750,]
wine_test <- wwine[3751:4898,]
```

### Training

we use the `rpart` library for it's "most faithful" implementation of regression trees.

```{r}
library(rpart)

m.rpart <- rpart(
    quality ~ ., 
    data = wine_train
)

m.rpart

#summary(m.rpart)
```

Alcohol level is the first split, so we infer it's the *most* important feature.

using the `rpart.plot` package, we can visualize the tree.

```{r}
rpart.plot::rpart.plot(m.rpart,
                       digits = 4,
                       fallen.leaves = TRUE,
                       type = 3,
                       extra = 101)
```

### evaluating performance

```{r}
p.rpart <- predict(m.rpart, wine_test)

summary(p.rpart)
```

```{r}
summary(wine_test$quality)
```

p.rpart seems to be missing the extremes.

```{r}
cor(p.rpart, wine_test$quality)
```

### measure performance with Mean Absolute Error

```{r}
# function to calculate the mean absolute error
MAE <- function(actual, predicted) {
  mean(abs(actual - predicted))  
}
```

```{r}
MAE(p.rpart, wine_test$quality)
```

```{r}
mean(wine_train$quality)
```

### Improving performance

-   **cubist algorithm**: current leader in model trees. Has a different syntax for building models.

```{r}
m.cubist <- 
    Cubist::cubist(x = wine_train[-12], # use all fields except #12, the quality field. 
                   y = wine_train$quality)

m.cubist
```

```{r}
summary(m.cubist)
```

use the model to predict!

```{r}
p.cubist <- predict(m.cubist, 
                    wine_test)

summary(p.cubist)
```

Ah ok, this shows more values toward the extremes.

```{r}
cor(p.cubist, wine_test$quality)
MAE(wine_test$quality, p.cubist)
```
