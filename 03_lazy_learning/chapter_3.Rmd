---
title: "chapter_3, Nearest Neighbors"
author: "jason grahn"
date: "2023-11-15"
output: github_document
---

```{r setup, include=FALSE, echo=FALSE}
library(tidyverse)
```

> Traditionally, the k-NN algorithm uses Euclidean distance, which is the distance one would measure if it were possible to use a ruler to connect two points. Euclidean distance is measured “as the crow flies,” which implies the shortest direct route. 

> The decision of how many neighbors to use for k-NN determines how well the model will generalize to future data.

> The balance between overfitting and underfitting underfitting the training data is a problem known as the **bias-variance tradeoff**.

Begin with a _k_ equal to the square root of the number of training examples. (Not a hard rule, but a fast rule.)

the larger the training dataset, the larger the pool of neighbor examples, the easier to classify. 

# Preparing data for use with k-NN

> Features are typically transformed to a standard range prior to applying the k-NN algorithm.

aka: Normalize or standardize your variables that go into training. If some variables have a huge range while others do not, then _k_-NN is more heavily influenced by the larger-ranged variables. 

There are lots of ways to do this: 

* min-max normalization
    + transforms items to a scale from 0-1 (0 - 100%)
    + $X_{new} = \frac{X-min(X)}{max(X)-min(X)}$
    + problem's arise if test data has values beyond the min/max evaluated, so need to take theoreticals into account.

* z-score standardization
    + how many standard deviations above or below the mean
    + assumes normally distributed data.
    + no predefined min and max values, so could float _anywhere_ 

* dummy coding
    + converting categories into _binary_ variables
    + case when X then 1 else 0 (I like this one quite a bit, very useful)

* "one-hot encoding" 
    + more conversionof categories into binary variables
    + case when hot then 1 = hot else 0, when medium then 1 else 0, else cold
    + this creates multiple binaries
    + one-hot makes problems with linear-regression. 
    + however, DOES make models easier to understand.

# what is "lazy"

systems that merely store training verbatim. You tell it what to do, it goes and does it. Heavy reliance on training rather than building an abstracted model is called "instance-based learning". And because it doesn't build a model it's called "non-parametric" because it doesn't learn about any of the parameters. 

# Example – diagnosing breast cancer with the k-NN algorithm

What we're doing to do is the whole chapter normally, then if we feel like doing it in tidy style, a redo section..

## base R

### step 1 collection 

```{r}
wbcd_tidy <- read_csv(here::here("03_lazy_learning", "data", "wisc_bc_data.csv")) %>% 
    janitor::clean_names() %>% 
    mutate(diagnosis = factor(diagnosis, levels = c("B", "M"),
                              labels = c("Benign", "Malignant"))
    )

wbcd <- read.csv("data/wisc_bc_data.csv")[-1]

wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
                         labels = c("Benign", "Malignant"))
```

### step 2 exploration and data prep 

```{r}
table(wbcd$diagnosis)
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])

```

the data ranges for these values are massively different. it's going to be hard to do any kNN on them. need to normalize them somehow. 

Oh, the book gives us a function for min-max normalization. 

```{r}
normalize <- function(x) {
    return ((x - min(x)) / (max(x) - min(x)))
}

normalize(c(1,2,3,4,5))
```

```{r}
normalize(c(10,20,30,40,50))
```

we're going to use `lapply` to apply the `normalize` function we just made to each numeric feature in the dataset. `lapply` and it's tidyverse counterpart `map` are a bit mysterious to me, so i'll use the code from the book. 

```{r}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n$area_mean)
```

Now we're going to divide the data into training and testing data. I know there's a tidy version of doing this too. 

```{r}
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
```

```{r}
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
```

### step 3 training 
```{r}
library(class)
wbcd_test_pred <- knn(train = wbcd_train,  #469 rows
                      test = wbcd_test,
                      cl = wbcd_train_labels, #469 rows
                      k = 21)
```

getting error: 
> Error in knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels[, : 'train' and 'class' have different lengths

error fixed by adding `$diagnosis` to the `cl` parameter. 

### step 4 model performance

```{r}
library(gmodels) #for evaluating model performance
```

```{r}
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
             prop.chisq = FALSE)
```

(I really dont like this format for results. Partly because I'm not used to seeing them this way.)

Top left is true negative. 61% of values sit here. 
Bottom right = true positive. 37%.
lower left = false negative. 2% predicted benign but actually malignant. 
top right, false positive. 0%

67+37+2 = 100%

### step 6 improvments

let's z-score standardize and see if that helps. 

```{r}
#standarization
wbcd_z <- as.data.frame(scale(wbcd[-1]))

#example summary stats.
summary(wbcd_z$area_mean)
```

The mean of z-score standardization should always be zero, and here it is. on the low side, nothing smaller than -3; on the high side we definitely have outliers.

regardless, we follow the same process as before, just now with the z-score normalized data.

```{r}
wbcd_train <- wbcd_z[1:469, ]
wbcd_test <- wbcd_z[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
wbcd_test_pred <- 
    knn(train = wbcd_train, 
        test = wbcd_test,
        cl = wbcd_train_labels, k = 21)

CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
             prop.chisq = FALSE)
```


Now we have 61 + 34 = 95% classified correctly; so the z-scoring normalization made things worse. having 5% misclassified as _false negatives_ is a big problem when it comes to tumors! 

OK, but what if we change how many neighbors we're using for sampling? (PS: part of the reason i dont like this method _so far_ is because we dont have an visualizations showing the classification objects, just an output table!)

they give us a for-loop code to run through different k-values. 

```{r}
k_values <- c(1, 5, 11, 15, 21, 27)

for (k_val in k_values) {
    wbcd_test_pred <- knn(train = wbcd_train,
                          test = wbcd_test,
                          cl = wbcd_train_labels,
                          k = k_val)
    CrossTable(x = wbcd_test_labels,
               y = wbcd_test_pred,
               prop.chisq = FALSE)
  }

```


## Tidy styling

Ohh, right, `mutate_if` let's me use the normalize function against the data. But ideally these would be _new_ columns just in case we want to look at the original data. 

the tidy way to do this would be to retain the ID column, then normalize as a mutate, then `slice_head` to get the same rows. 
```{r}
cancer_tidy <- 
    wbcd_tidy %>% 
    mutate_if(is.double, normalize) 

cancer_tidy_train <- 
    cancer_tidy %>% 
    slice_head(n = 469)

cancer_tidy_test <- 
    cancer_tidy %>% 
    anti_join(cancer_tidy_train, by = 'id')
```

I like the tidy way better because it maintains the other variables and we dont need to rejoin to get the diagnosis info. But I digress, let's make data frames of the diagnosis column data...

```{r}
cancer_predict <- knn(train = cancer_tidy_train %>% select(-c(1:2)),  #469 rows
    test = cancer_tidy_test %>% select(-c(1:2)),
    cl = cancer_tidy_train$diagnosis, #469 rows
    k = 21)
```

At the end of the chapter, we have implication that we're going to be doing things a bit differently later on, so I'll save my tidy translations until I see otherwise. Maybe I'll just get another book of the same topic that does it all Tidy. 
